<p align="center"><h1 align="center">APPLYBN</h1></p>
<p align="center">
	<img src="https://img.shields.io/github/license/Anaxagor/applybn?style=default&logo=opensourceinitiative&logoColor=white&color=blue" alt="license">
</p>
<p align="center">Built with the tools and technologies:</p>
<p align="center">
	<img src="https://img.shields.io/badge/Python-3776AB.svg?style=default&logo=Python&logoColor=white"alt="Python">
	<img src="https://img.shields.io/badge/GitHub%20Actions-2088FF.svg?style=default&logo=GitHub-Actions&logoColor=white"alt="GitHub%20Actions">
</p>
<br>

applybn is an open-source multi-purpose framework based on Bayesian networks and Causal models.
It introduces data analysis based on understandable and interpretable algorithms of Bayesian networks and causal models.
![image](https://github.com/user-attachments/assets/996f8e5a-1742-4849-a64f-58b97a4cf17d)

## Key Features
### 1. **Anomaly Detection in time-series and tabular data**
#### **Tabular data**
   - The method combines detection based on Bayesian network conditional distributions and proximity-based scoring.
   - The method allows capturing both density outliers and outliers generated by violation of dependencies between features. 
#### **Time series**
   -  The method identifies outliers in multivariate time series by leveraging dynamic Bayesian networks (DBNs) to model temporal and cross-variable dependencies.
   -  Its key strengths lie in capturing complex temporal interdependencies, automating outlier classification through adaptive score-analysis strategies. 
### 2. **Synthetic Data Generation**
   - The framework includes methods for generating synthetic training data when class imbalance is detected in the dataset. Using hybrid Bayesian networks (with Gaussian mixture models), it generates balanced synthetic data, improving model training outcomes.
   - The method allows us to account for interactions between variables, which improves the quality of the synthetics.

### 3. **Features Selection and Extraction**
#### **Causal effect based features selection**
   - This method quantifies causal effects via information theory and post-nonlinear models, automatically selecting features with non-zero causal impact on key performance indicators (KPIs) by analyzing uncertainty reduction in entropy.
   - The method excels in interpretability and stability by prioritizing causal relationships, avoids manual threshold tuning, and performs reliably even with limited or nonlinear industrial data, enhancing soft sensor robustness.
#### **Normalized MI based features selection**
   - The method combines feature selection with Meek rules to identify Markov blankets, using mutual information to retain nodes with significant dependencies while pruning irrelevant or redundant features in Bayesian networks.
   - Its key advantages include efficient handling of high-dimensional data by focusing on local structures, improved accuracy over traditional methods.
#### **Bayesian networks based features extraction**
   - The method operates by inferring conditional dependencies between features using Bayesian networks and appending probabilistic parameters (Î») representing these relationships to the original dataset.
   - Its advantages include enhanced classification accuracy across diverse domains, reduced reliance on prior knowledge, and low computational complexity, enabling robust and interpretable feature enrichment without domain-specific constraints.

### 4. **Explainable Analysis**
#### **Causal Analysis for Machine Learning Models**
   - The method of analyzing model components - a structural causal model (SCM) is built to analyze deep learning models, allowing for the pruning of unimportant parts (e.g., filters in CNNs) by evaluating their causal importance.
   - The method of explaining data impact on predictions allows for causal inference between features and the model's confidence scores. By calculating the **Average Causal Effect (ACE)**, it helps identify which features significantly influence model uncertainty, providing valuable insights for improving or debugging models.

### 5. **Scikit-learn compatible**
All the estimators and data transformers are scikit-learn compatible.

## Installation

To get started with applybn, clone the repository and install the required dependencies.

```bash
git clone https://github.com/Anaxagor/applybn.git
cd applybn
poetry install
```

## Scikit-learn Pipeline example

An example demonstrating how to use several components from the `applybn` library in a scikit-learn compatible pipeline. We'll cover feature selection, feature generation, oversampling for imbalanced datasets, and anomaly detection.

```python
import pandas as pd
import numpy as np
import warnings
import logging
import sys
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import KBinsDiscretizer
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.linear_model import LogisticRegression
from applybn.feature_selection.ce_feature_select import CausalFeatureSelector
from applybn.feature_extraction.bn_feature_extractor import BNFeatureGenerator
from applybn.imbalanced.over_sampling.bn_over_sampler import BNOverSampler
from applybn.anomaly_detection.static_anomaly_detector.tabular_detector import TabularDetector
warnings.filterwarnings('ignore')
logging.getLogger().setLevel(logging.CRITICAL)
logging.disable(logging.CRITICAL)
logging.getLogger('applybn').setLevel(logging.CRITICAL)
logging.getLogger('applybn').disabled = True
class NullWriter:
    def write(self, txt): pass
    def flush(self): pass
original_stderr = sys.stderr
sys.stderr = NullWriter()

class CausalFeatureSelector2(CausalFeatureSelector):
    def fit(self, X, y):
        super().fit(X, y)
        self.selected_features_mask_ = self.get_support()
        return self
        
    def transform(self, X):
        return X.iloc[:, self.selected_features_mask_]

def generate_example_data(n_samples=200, n_features=5, n_cat_features=2, target_name='target_class', imbalance_ratio=0.1, random_state=42):
    """Generates a synthetic imbalanced dataset."""
    rng = np.random.RandomState(random_state)
    X_cont_data = rng.rand(n_samples, n_features - n_cat_features)
    cont_feature_names = [f'cont_feature_{j}' for j in range(n_features - n_cat_features)]
    X_cont_df = pd.DataFrame(X_cont_data, columns=cont_feature_names)
    
    X_cat_df = pd.DataFrame()
    for i in range(n_cat_features):
        cat_feature_name = f'cat_feature_{i}'
        temp_cont_for_cat = rng.rand(n_samples)
        n_bins_cat = rng.randint(2, 4)
        discretizer = KBinsDiscretizer(n_bins=n_bins_cat, encode='ordinal', strategy='uniform', subsample=None, random_state=rng)
        # Keep categorical features as integers
        X_cat_df[cat_feature_name] = discretizer.fit_transform(temp_cont_for_cat.reshape(-1, 1)).ravel().astype(int)
    X_df = pd.concat([X_cont_df, X_cat_df], axis=1)
    
    n_class1 = int(n_samples * imbalance_ratio)
    n_class0 = n_samples - n_class1
    y_array = np.array([0] * n_class0 + [1] * n_class1)
    rng.shuffle(y_array)
    y_series = pd.Series(y_array, name=target_name, dtype=int)
    return X_df, y_series

# --- Main Example ---
TARGET_NAME = 'target_class_example'

# 1. Generate Data
print("1. Generating Synthetic Data...")
X_df, y_s = generate_example_data(n_samples=250, n_features=6, n_cat_features=2, target_name=TARGET_NAME, imbalance_ratio=0.15, random_state=42)
X_train_df, X_test_df, y_train_s, y_test_s = train_test_split(X_df, y_s, test_size=0.3, random_state=42, stratify=y_s)
print(f"   Train data: {X_train_df.shape}, Test data: {X_test_df.shape}")
print(f"   Target distribution (train):\n{y_train_s.value_counts(normalize=True).to_string()}\n")

# 2. Process data with anomaly detection
X_train_df = X_train_df.reset_index(drop=True)
y_train_s = y_train_s.reset_index(drop=True)
X_test_df = X_test_df.reset_index(drop=True)
y_test_s = y_test_s.reset_index(drop=True)
detector = TabularDetector(target_name=TARGET_NAME, additional_score="IF")
X_train_df[TARGET_NAME] = y_train_s
detector.fit(X_train_df)
scores = detector.predict_scores(X_train_df)
threshold = np.percentile(scores, 95)
mask = scores <= threshold

X_train_df = X_train_df[mask]
y_train_s = X_train_df[TARGET_NAME]
y_train_s = y_train_s.reset_index(drop=True)
X_train_df = X_train_df.drop(columns=[TARGET_NAME])
X_train_df = X_train_df.reset_index(drop=True)

# 3. Create the full pipeline with feature selection, feature generation, oversampling, and classification
processing_pipeline = ImbPipeline([
    ('feature_selector', CausalFeatureSelector2(n_bins=3)),
    ('bn_features', BNFeatureGenerator()),
    ('oversampler', BNOverSampler(class_column=TARGET_NAME, strategy='max_class', shuffle=True)),
    ('classifier', LogisticRegression(solver='liblinear', random_state=42))
])

# 4. Fit and evaluate the pipeline
print("\n2. Fitting complete pipeline...")
processing_pipeline.fit(X_train_df, y_train_s)
pipeline_score = processing_pipeline.score(X_test_df, y_test_s)

print(f"   Pipeline test accuracy: {pipeline_score:.4f}\n")

print("\n--- Example Finished ---")
```

## Help and Support

### Documentation with examples

The documentation with examples can be found [here](https://anaxagor.github.io/applybn/).

### Communitcation

Feel free to create new [issues](https://github.com/Anaxagor/applybn/issues).

If you have questions or suggestions, you can contact us at the following resources:
* [Telegram chat helpdesk](https://t.me/+4FOcyF0Rri00ZGEy)
* ideeva@itmo.ru (Irina Deeva) 

### Contributing

Contributions to applybn are welcome! If you're interested in improving any of the features or testing new branches, please see the [How to contribute](https://anaxagor.github.io/applybn/development/contributing/) section of the documentation.

### License

applybn is distributed under the MIT License. See the `LICENSE` file for more information.

## Citation

```bibtex
@software{applybn,
  author = {Irina Deeva},
  title = {applybn},
  url = {https://github.com/Anaxagor/applybn},
  version = {0.1.0},
  date = {2025-05-03},
}
```

## Acknowledgement
The project is supported by [FASIE](https://fasie.ru/) - Foundation for Assistance to Small Innovative Enterprises.
