# CausalFeatureSelector

## Обзор

Класс [`CausalFeatureSelector`](../../api/feature_selection/causal_feature_selection.md) реализует метод отбора признаков,
вдохновленный каузальными моделями, для выявления признаков со значительным каузальным влиянием на целевую переменную.
Этот подход основан на методологии, описанной в статье
["A Causal Model-Inspired Automatic Feature-Selection Method for Developing Data-Driven Soft Sensors in Complex Industrial Processes"](https://www.sciencedirect.com/science/article/pii/S2095809922005641).
Вместо того чтобы полагаться на корреляцию, он отбирает признаки, количественно оценивая их каузальное влияние через
снижение энтропии, обеспечивая интерпретируемые и каузально релевантные подмножества признаков.

## Ключевые особенности

- **Оценка каузального эффекта**: оценивает признаки на основе их способности снижать неопределенность в целевой переменной.
- **Дискретизация по IQR**: автоматически разбивает непрерывные признаки на интервалы с использованием правила межквартильного размаха (IQR).
- **Интеграция со Scikit-Learn**: совместим с конвейерами и преобразователями scikit-learn.

## Математическое обоснование

### Дискретизация

Признаки и целевая переменная дискретизируются с использованием разбиения на основе IQR. Количество интервалов `n_bins` рассчитывается как:

\[ n_{\text{bins}} = \max\left(2, \left\lceil \frac{R}{2 \cdot \text{IQR} \cdot n^{1/3}} \cdot \log_2(n + 1) \right\rceil \right) \]

где \( R \) — это диапазон данных, \( \text{IQR} \) — межквартильный размах, а \( n \) — размер выборки.

### Расчет каузального эффекта

Каузальный эффект признака \( X_i \) на цель \( Y \) вычисляется как уменьшение условной энтропии:

\[\text{CE}(X_i \rightarrow Y) = H(Y \mid \text{другие признаки}) - H(Y \mid X_i, \text{другие признаки})\]

где \( H \) обозначает энтропию. Сохраняются признаки с \( \text{CE} > 0 \).

## Пример

``` py title="examples/feature_selection/causal_fs_example.py"
--8<-- "examples/feature_selection/causal_fs_example.py"
```
